{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **What are the best cases for using violin?**\n",
    "    - You usually want to use a violin plot if you have a categorical/ordinal feature vs. a continuous feature and the categorical/ordinal feature has maybe more than 3-4 categories so overlapping histograms won't work.\n",
    "- **\"There is no objective ground truth for several types of vars, I think that might be something more I would have to think about when trying to tell a story!**\n",
    "    - While that's certianly true sometimes, it's usually not too difficult to decide the type of a variable.\n",
    "    - If you are in doubt, try both data types in your ML pipeline and check if it has impact on the performance of your ML model\n",
    "- **Too many parameters can be played with in the plot function!**\n",
    "    - YES! And I only had time to show what I believe are the most imnportant ones!\n",
    "    - Please read the manual to see all the parameters.\n",
    "    - Your figure quality will greatly improve if you are aware and use the functionalities and you are intentional when you prepare figures\n",
    "- **Both the box plot and the violin plot are used to visualize categorical v.s. continuous variables, so what should we consider while choosing one of these two types of plots (what is the tradeoff of picking one over the other)?**\n",
    "    - Preparing visualizations can be subjective. In this case, I'd say that either a box and a violin plot works well, it's up to you to decide which one you like more.\n",
    "- **I have not used violin plots in other courses, so I am curious about what purpose they hold and when is most appropriate to use.**\n",
    "    - They are a good alternative for box plots if you want to see the histogram of the continuous feature.\n",
    "- **I found continuous vs categorical and categorical vs continuous use different visualization, so how do we decided if it is cont vs cate or cate vs cont given two features?**\n",
    "    - Nope, check the 2x2 matrix again.\n",
    "    - The same figures should be used for either, the order of the features don't matter.\n",
    "- **Are we supposed to memorize all specific implementations of the visualizing tools?**\n",
    "    - The syntax, no. You can always look up the manual.\n",
    "    - The visualization types, when to use them, when to use e.g., log axes, etc, that's what you need to know.\n",
    "- **how to determine the appropriate number of bins.**\n",
    "    - That's dataset specific so just experiment with a few values and see what's best.\n",
    "- **Was a bit overwhelmed by content - would like more practice with guided creation of plts.**\n",
    "    - You'll practice it in PS3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Lecture 5: Data splitting, part 1</center>\n",
    "\n",
    "## Split iid data\n",
    "By the end of this lecture, you will be able to\n",
    "- describe what the iid assumption is\n",
    "- apply basic split to iid datasets\n",
    "- apply k-fold split to iid datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The supervised ML pipeline\n",
    "\n",
    "**0. Data collection/manipulation**: you might have multiple data sources and/or you might have more data than you need\n",
    "   - you need to be able to read in datasets from various sources (like csv, excel, SQL, parquet, etc)\n",
    "   - you need to be able to filter the columns/rows you need for your ML model\n",
    "   - you need to be able to combine the datasets into one dataframe \n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "<span style=\"background-color: #FFFF00\">**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)</span>\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to be transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation or hyperparameter tuning)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- **describe what the iid assumption is**\n",
    "- <font color='lightgray'>apply basic split to iid datasets</font>\n",
    "- <font color='lightgray'>apply k-fold split to iid datasets</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's revisit the papaya example from the first lecture!\n",
    "\n",
    "- **the learner's input:**\n",
    "    - Domain set $\\mathcal{X}$ - a set of objects we wish to label (*all papayas on the island*).\n",
    "    - The probability distribution over $\\mathcal{X}$ is $D$.\n",
    "        - $D$ is pretty general\n",
    "        - $D$ might change as you sample from it\n",
    "        - there might be multiple distributions you sample from\n",
    "        - one sample might depend on one or multiple previous samples, etc.\n",
    "    - Label set $\\mathcal{Y}$ - a set of possible labels (*a papaya can be either tasty or not tasty*). \n",
    "    - There is some correct labeling function $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$. \n",
    "    - **a training example is then generated by sampling $x_i$ from $D$, and the label $y_i$ is generated using $f$.**\n",
    "    - Training data $S = ((x_1, y_1),...,(x_m,y_m))$ - a finite sequence of pairs from $\\mathcal{X}$, $\\mathcal{Y}$. This is what the learner has access to (*the data I collected by sampling some papayas*).\n",
    "        - $X = (x_1,...,x_m)$ is the feature matrix which is usually a 2D matrix, and $Y = (y_1,...,y_m)$ is the target variable which is a vector.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.I.D. assumption\n",
    "\n",
    "- **the i.i.d. assumption**: the examples in the training set are independently and identically distributed according to $D$\n",
    "    - every $x_i$ is freshly sampled from $D$ and then labelled by $f$\n",
    "    - that is, $x_i$ and $y_i$ are picked independently of the other instances\n",
    "    - $S$ is a window through which the learner gets partial info about $D$ and the labeling function $f$\n",
    "    - the larger the sample gets, the more likely it is that $D$ and $f$ are accurately reflected\n",
    "- examples of not iid data:\n",
    "   - data generated by time-dependent processes\n",
    "   - data has group structure (samples collected from e.g., different subjects, experiments, measurement devices)\n",
    "   - sampling from the distribution changes the properties of the distribution\n",
    "- we will get back to this later in the term "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1\n",
    "Which of these data generation processes or ML problems are not iid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>describe what the iid assumption is</font>\n",
    "- **apply basic split to iid datasets**\n",
    "- <font color='lightgray'>apply k-fold split to iid datasets</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we split the data?\n",
    "- we want to find the best hyper-parameters of our ML algorithms\n",
    "   - fit models to training data\n",
    "   - evaluate each model on validation set\n",
    "   - we find hyper-parameter values that optimize the validation score\n",
    "- we want to know how the model will perform on previously unseen data\n",
    "   - apply our final model on the test set\n",
    "   \n",
    "### We need to split the data into three parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting strategies for iid data: basic approach\n",
    "- 60% train, 20% validation, 20% test for small datasets\n",
    "- 98% train, 1% validation, 1% test for large datasets\n",
    "    - if you have 1 million points, you still have 10000 points in validation and test which is plenty to assess model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's work with the adult data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         <=50K\n",
      "1         <=50K\n",
      "2         <=50K\n",
      "3         <=50K\n",
      "4         <=50K\n",
      "          ...  \n",
      "32556     <=50K\n",
      "32557      >50K\n",
      "32558     <=50K\n",
      "32559     <=50K\n",
      "32560      >50K\n",
      "Name: gross-income, Length: 32561, dtype: object\n",
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country  \n",
      "0          2174             0              40   United-States  \n",
      "1             0             0              13   United-States  \n",
      "2             0             0              40   United-States  \n",
      "3             0             0              40   United-States  \n",
      "4             0             0              40            Cuba  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('../data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "print(y)\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(\n",
      "    *arrays,\n",
      "    test_size=None,\n",
      "    train_size=None,\n",
      "    random_state=None,\n",
      "    shuffle=True,\n",
      "    stratify=None\n",
      ")\n",
      "    Split arrays or matrices into random train and test subsets.\n",
      "\n",
      "    Quick utility that wraps input validation,\n",
      "    ``next(ShuffleSplit().split(X, y))``, and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data into a\n",
      "    one-liner.\n",
      "\n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "\n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "\n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "\n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "\n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "\n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "\n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "\n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n",
      "    >>> from sklearn import datasets\n",
      "    >>> iris = datasets.load_iris(as_frame=True)\n",
      "    >>> X, y = iris['data'], iris['target']\n",
      "    >>> X.head()\n",
      "        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "    0                5.1               3.5                1.4               0.2\n",
      "    1                4.9               3.0                1.4               0.2\n",
      "    2                4.7               3.2                1.3               0.2\n",
      "    3                4.6               3.1                1.5               0.2\n",
      "    4                5.0               3.6                1.4               0.2\n",
      "    >>> y.head()\n",
      "    0    0\n",
      "    1    0\n",
      "    2    0\n",
      "    3    0\n",
      "    4    0\n",
      "    ...\n",
      "\n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ... X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train.head()\n",
      "        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "    96                 5.7               2.9                4.2               1.3\n",
      "    105                7.6               3.0                6.6               2.1\n",
      "    66                 5.6               3.0                4.5               1.5\n",
      "    0                  5.1               3.5                1.4               0.2\n",
      "    122                7.7               2.8                6.7               2.0\n",
      "    >>> y_train.head()\n",
      "    96     1\n",
      "    105    2\n",
      "    66     1\n",
      "    0      0\n",
      "    122    2\n",
      "    ...\n",
      "    >>> X_test.head()\n",
      "        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "    73                 6.1               2.8                4.7               1.2\n",
      "    18                 5.7               3.8                1.7               0.3\n",
      "    118                7.7               2.6                6.9               2.3\n",
      "    78                 6.0               2.9                4.5               1.5\n",
      "    76                 6.8               2.8                4.8               1.4\n",
      "    >>> y_test.head()\n",
      "    73     1\n",
      "    18     0\n",
      "    118    2\n",
      "    78     1\n",
      "    76     1\n",
      "    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all sklearn transformers and models accept polars dataframes!\n",
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (19536, 14) (19536,)\n",
      "(13025, 14) (13025,)\n",
      "validation set: (6512, 14) (6512,)\n",
      "test set: (6513, 14) (6513,)\n",
      "       age   workclass  fnlwgt      education  education-num  \\\n",
      "25823   31     Private   87418      Assoc-voc             11   \n",
      "10274   41     Private  121718   Some-college             10   \n",
      "27652   61     Private   79827        HS-grad              9   \n",
      "13941   33   State-gov  156015      Bachelors             13   \n",
      "31384   38     Private  167882   Some-college             10   \n",
      "\n",
      "            marital-status        occupation     relationship    race  \\\n",
      "25823   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "10274   Married-civ-spouse      Craft-repair          Husband   White   \n",
      "27652   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "13941   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "31384              Widowed     Other-service   Other-relative   Black   \n",
      "\n",
      "           sex  capital-gain  capital-loss  hours-per-week  native-country  \n",
      "25823     Male             0             0              40   United-States  \n",
      "10274     Male             0             0              40           Italy  \n",
      "27652     Male             0             0              50   United-States  \n",
      "13941     Male             0             0              40   United-States  \n",
      "31384   Female             0             0              45           Haiti  \n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,\\\n",
    "                    train_size = 0.6,random_state = random_state)\n",
    "print('training set:',X_train.shape, y_train.shape) # 60% of points are in train\n",
    "print(X_other.shape, y_other.shape) # 40% of points are in other\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,\\\n",
    "                    train_size = 0.5,random_state = random_state)\n",
    "print('validation set:',X_val.shape, y_val.shape) # 20% of points are in validation\n",
    "print('test set:',X_test.shape, y_test.shape) # 20% of points are in test\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomness due to splitting\n",
    "- the model performance, validation and test scores will change depending on which points are in train, val, test\n",
    "    - inherent randomness or uncertainty of the ML pipeline\n",
    "- change the random state a couple of times and repeat the whole ML pipeline to assess how much the random splitting affects your test score\n",
    "    - you would expect a similar uncertainty when the model is deployed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2\n",
    "\n",
    "What's the second train_test_split line if you want to end up with 60-20-20 in train-val-test? Print out the sizes of X_train, X_val, X_test to verify!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (19536, 14) (19536,)\n",
      "validation set: (6512, 14) (6512,)\n",
      "test set: (6513, 14) (6513,)\n"
     ]
    }
   ],
   "source": [
    "X_other, X_test, y_other, y_test = train_test_split(X,y,\\\n",
    "                    train_size = 0.8,random_state=random_state)\n",
    "# add your line below and choose the correct solution from canvas\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_other, y_other,\\\n",
    "                                                  train_size = 0.75,random_state=random_state) # test_size = 0.25 also works\n",
    "print('training set:',X_train.shape, y_train.shape)\n",
    "print('validation set:',X_val.shape, y_val.shape)\n",
    "print('test set:',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>describe what the iid assumption is</font>\n",
    "- <font color='lightgray'>apply basic split to iid datasets</font>\n",
    "- **apply k-fold split to iid datasets**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other splitting strategy for iid data: k-fold splitting\n",
    "\n",
    "<center><img src=\"../figures/grid_search_cross_validation.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_UnsupportedGroupCVMixin, _BaseKFold)\n",
      " |  KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      " |\n",
      " |  K-Fold cross-validator.\n",
      " |\n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |\n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <k_fold>`.\n",
      " |\n",
      " |  For visualisation of cross-validation behaviour and\n",
      " |  comparison between common scikit-learn split methods\n",
      " |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of folds. Must be at least 2.\n",
      " |\n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``n_splits`` default value changed from 3 to 5.\n",
      " |\n",
      " |  shuffle : bool, default=False\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |      Note that the samples within each split will not be shuffled.\n",
      " |\n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      " |      indices, which controls the randomness of each fold. Otherwise, this\n",
      " |      parameter has no effect.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
      " |  ...     print(f\"Fold {i}:\")\n",
      " |  ...     print(f\"  Train: index={train_index}\")\n",
      " |  ...     print(f\"  Test:  index={test_index}\")\n",
      " |  Fold 0:\n",
      " |    Train: index=[2 3]\n",
      " |    Test:  index=[0 1]\n",
      " |  Fold 1:\n",
      " |    Train: index=[0 1]\n",
      " |    Test:  index=[2 3]\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |\n",
      " |  Randomized CV splitters may return different results for each call of\n",
      " |  split. You can make the results identical by setting `random_state`\n",
      " |  to an integer.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  StratifiedKFold : Takes class information into account to avoid building\n",
      " |      folds with imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |\n",
      " |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      " |\n",
      " |  RepeatedKFold : Repeats K-Fold n times.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _UnsupportedGroupCVMixin\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      " |\n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |\n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |\n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26048, 14) (26048,)\n",
      "test set: (6513, 14) (6513,)\n",
      "   training set: (20838, 14) (20838,)\n",
      "   validation set: (5210, 14) (5210,)\n",
      "       age   workclass      education\n",
      "27240   38     Private      Bachelors\n",
      "4       28     Private      Bachelors\n",
      "14242   34     Private        HS-grad\n",
      "16461   58     Private   Some-college\n",
      "2209    49   Local-gov        HS-grad\n",
      "   training set: (20838, 14) (20838,)\n",
      "   validation set: (5210, 14) (5210,)\n",
      "       age   workclass   education\n",
      "5514    33   Local-gov   Bachelors\n",
      "32240   21     Private   Assoc-voc\n",
      "8615    33     Private        10th\n",
      "7743    20     Private     HS-grad\n",
      "20097   39     Private   Assoc-voc\n",
      "   training set: (20838, 14) (20838,)\n",
      "   validation set: (5210, 14) (5210,)\n",
      "       age          workclass      education\n",
      "9876    27            Private   Some-college\n",
      "5455    44            Private      Bachelors\n",
      "29805   62   Self-emp-not-inc      Bachelors\n",
      "15081   20            Private        HS-grad\n",
      "13770   40            Private     Assoc-acdm\n",
      "   training set: (20839, 14) (20839,)\n",
      "   validation set: (5209, 14) (5209,)\n",
      "       age          workclass      education\n",
      "19777   36            Private      Assoc-voc\n",
      "10781   58   Self-emp-not-inc            9th\n",
      "9747    24            Private      Bachelors\n",
      "327     43            Private   Some-college\n",
      "24431   25            Private        HS-grad\n",
      "   training set: (20839, 14) (20839,)\n",
      "   validation set: (5209, 14) (5209,)\n",
      "       age workclass     education\n",
      "17203   33   Private       HS-grad\n",
      "12114   36   Private   Prof-school\n",
      "231     41   Private       HS-grad\n",
      "3272    30   Private          10th\n",
      "26009   19   Private          11th\n"
     ]
    }
   ],
   "source": [
    "random_state =42\n",
    "\n",
    "# first split to separate out the test set\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "print(X_other.shape,y_other.shape)\n",
    "print('test set:',X_test.shape,y_test.shape)\n",
    "\n",
    "# do KFold split on other\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print('   training set:',X_train.shape, y_train.shape) \n",
    "    print('   validation set:',X_val.shape, y_val.shape) \n",
    "    # the validation set contains different points in each iteration\n",
    "    print(X_val[['age','workclass','education']].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How many splits should I create?\n",
    "- tough question, 3-5 is most common\n",
    "- if you do n splits, n models will be trained, so the larger the n, the most computationally intensive it will be to train the models\n",
    "- KFold is usually better suited to small datasets\n",
    "- KFold is good to estimate uncertainty due to random splitting of train and val, but it is not perfect\n",
    "    - the test set remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why shuffling iid data is important?\n",
    "- by default, data is not shuffled by Kfold which can introduce errors!\n",
    "<center><img src=\"../figures/kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (data1030)",
   "language": "python",
   "name": "data1030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
