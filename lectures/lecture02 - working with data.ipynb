{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard\n",
    "- **The muddiest part for me was classifying different problems (regression, binary, etc). It isn't always clear which approach is best.**\n",
    "- **I struggled with the classification of which type of target variable to use in different situations**\n",
    "    - Yes, that's a pretty tough question and there is no \"one answer fits all\" solution generally because a problem can be phrased as various different supervised ML problems.\n",
    "    - Always consider what you plan to do with the ML model once it is deployed and figure out what sort of supervised ML problem best suits your needs.\n",
    "- **I'm still a bit confused about the ML pipeline.**\n",
    "- **I think the ML pipeline was the most muddiest just out out familiarity**\n",
    "    - We will go through each step in detail so don't worry about it.\n",
    "- **What to choose for the project? What are the stuffs I need to consider for it.**\n",
    "    - I'll send out an announcement about this in a few days.\n",
    "- **For coding, this course use Python, but my undergraduate courses all used R.**\n",
    "    - We start slow so you could use the first few weeks of the term to pick up python.\n",
    "    - If that's not something you'd like to do, consider dropping the course.\n",
    "- **So for the final project, is it possible to show/give us an example of what is expected for us to show?**\n",
    "    - Yes, there is a final reports folder in the course repo which contains a couple of great final reports from previous years.\n",
    "- **I was struggling a bit with the feature matrix and understanding the difference between that and target variables.**\n",
    "    - The target variable is always the variable you want to predict with the ML model. All other variables are usually in the feature matrix.\n",
    "    - There are some exceptions to it like unique IDs or group IDs, we will cover those later.\n",
    "- **About the target variable concept, i believe including more example will be better for us to understand**\n",
    "    - I'd consider it a responsible use of GenAI to ask it for more examples of each type of supervised ML problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Lecture 2</center>\n",
    "## <center>Working with data (step 0)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started with Step 0!\n",
    "\n",
    "## The supervised ML pipeline\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">**0. Data collection/manipulation**: you might have multiple data sources and/or you might have more data than you need</span>\n",
    "   - you need to be able to read in datasets from various sources (like csv, excel, SQL, parquet, etc)\n",
    "   - you need to be able to filter the columns/rows you need for your ML model\n",
    "   - you need to be able to combine the datasets into one dataframe \n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to be transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation or hyperparameter tuning)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "By the end of the lecture, you will be able to\n",
    "- list main issues with data selection and collection\n",
    "- use pandas/polars to read in a dataset\n",
    "- filter rows of a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'> Learning objectives </font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to </font>\n",
    "- **list main issues with data selection and collection** \n",
    "- <font color='LIGHTGRAY'>use pandas/polars to read in a dataset </font>\n",
    "- <font color='LIGHTGRAY'>filter rows of a dataframe</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data selection and collection issues\n",
    "- the field is called **DATA** science for a reason!\n",
    "    - data is the most important part of data science\n",
    "    - the quality and quantity of data determines if a project is feasible\n",
    "    - it is usually much more valuable than algorithms\n",
    "    - working with data takes up ~80% of a data scientist's time\n",
    "- when you start working on a new project, approach your dataset with healthy skepticism!\n",
    "- ask questions in two main categories:\n",
    "    - is the data appropriate for the problem you are trying to solve?\n",
    "    - is the data accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the dataset appropriate for the purpose?\n",
    "- Can you answer your question with your data?\n",
    "    - medical studies based on white men only, can you use it to diagnose everyone?\n",
    "    - sometimes the answer is no! some diseases can differently impact man vs women or various racial groups\n",
    "    - sometimes the answer is yes! some diseases manifest the same way in everyone\n",
    "- Is your dataset timely?\n",
    "    - goal: predict covid cases today\n",
    "        - covid changed a lot over the years: spreads easier but symptoms are milder\n",
    "        - covid data from 2020 and 2021 might not be useable today\n",
    "    - goal: predict severe weather events like hurricanes\n",
    "        - there are not many hurricanes in each year so the temptation is to use data going back as far as possible\n",
    "        - hurricanes became more severe and more frequent in recent years\n",
    "        - is it OK to use data from e.g., 1960s to predict hurricanes today?\n",
    "- What biases are there in your dataset?\n",
    "    - your ML model will learn any biases your data has\n",
    "    - gender bias and racial bias are the main things to worry about when dealing with human data\n",
    "- is your dataset legal, ethical, and reliable?\n",
    "    - can you use the dataset legally?\n",
    "        - protected attributes (such as gender or race) often cannot be used especially in finance for example\n",
    "    - ethical usage\n",
    "        - if data is collected for one purpose, can you use it to solve another problem?\n",
    "    - reliability\n",
    "        - are there any conficts of interests that might make the dataset unreliable?\n",
    "        - example: climate data from big oil companies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the dataset accurate?\n",
    "- typos and errors\n",
    "    - mistakes by humans inputting data are extremely common!\n",
    "- Why are there missing values in the dataset?\n",
    "    - could be fine\n",
    "        - some respondents didn't answer all the survey questions\n",
    "        - doctor didn't perform test on all patients\n",
    "    - could be because of instrucment malfunction\n",
    "    - changes in data collection process over time\n",
    "- How are the missing values represented?\n",
    "    - sometimes as np.nan\n",
    "    - sometimes a string like 'missing' or '?'\n",
    "    - sometimes unreasonable values are used\n",
    "- Are the values valid?\n",
    "    - sometimes you'll see incorrect or impossible values\n",
    "    - 6 digit zip codes\n",
    "    - people older than 200 years\n",
    "    - negative numbers for a quantity that can only be non-negative\n",
    "- Duplicate records\n",
    "    - could be due to data entry error or data manipulation error (incorrect merge or append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "- document your dataset!\n",
    "- your future self and anyone else trying to reproduce your work will thank you!\n",
    "- can be as simple as a text file\n",
    "- describe each column in your dataset\n",
    "    - what is described in the column?\n",
    "    - what quantity is measued? does it have a unit?\n",
    "    - what's the range of valid values?\n",
    "    - what possible categories could there be? what does each category mean?\n",
    "    - are there missing values? if there are, why?\n",
    "    - how are the missing values represented?\n",
    "- you will see examples of this already today!\n",
    "- **if the dataset for your final project does not come with documentation, you need to write one!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'> Learning objectives </font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to </font>\n",
    "- <font color='LIGHTGRAY'>list main issues with data selection and collection</font>\n",
    "- **use pandas/polars to read in a dataset**\n",
    "- <font color='LIGHTGRAY'>filter rows of a dataframe</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas and Polars - why should you care?\n",
    "\n",
    "- **when you work on an ML problem, you might work with data from various sources**\n",
    "    - healthcare data might come from hospitals, insurance companies, state/federal agencies, etc.\n",
    "    - finance data could come from banks, brokerage accounts, social security office, etc.\n",
    "    - **you will need to pull data from all of these different sources and create one combined dataset ready for ML**\n",
    "- **you might also have more data than you need to solve the ML problem**\n",
    "    - in healthcare, you might be interested in people who have a certain symptom, or maybe you are interested in people who visited the ER multiple times\n",
    "    - in finance, you might be required by law to not use sensitive or protected attributes eventhough you have access to them\n",
    "    - **you need to filter out the rows and columns you need**\n",
    "- packages like pandas and polars make this easy for you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas and polars intro\n",
    "\n",
    "Similarities:\n",
    "- both are packages used for data manipulation and analysis\n",
    "- both use the concept of data frames and series - we will talk about this more later today!\n",
    "- both support reading and writing data in various formats (like CVS, excel, SQL, JSON, etc.)\n",
    "- syntax is similar -- polars uses syntax similar to pandas to make it easier for pandas users to switch over :)\n",
    "\n",
    "Differences:\n",
    "- pandas is more established (released in 2008) so it has a large userbase, great manuals, large community to help with issues\n",
    "- polars is a pretty new package, it has only been around since 2020 but the userbase is rapidly growing\n",
    "- pandas integrates with more packages than polars (although that's changing)\n",
    "- polars is much faster than pandas on large datasets\n",
    "\n",
    "When to use pandas?\n",
    "- Most companies have exensive code bases in pandas and it is unlikely they will switch over anytime soon, it's too costly.\n",
    "- If you work with small to medium datasets and computational speed and memory efficiency are not that critical, pandas is fine.\n",
    "\n",
    "When to use polars?\n",
    "- If you work on large datasets.\n",
    "- If computational speed and memory efficiency are mission-critical\n",
    "\n",
    "Keep in mind that you can use both packages to make the best of both worlds!\n",
    "- Use polars to perform the heavy computations!\n",
    "- Use polars.to_pandas() and polars.from_pandas() to convert dataframes as needed!\n",
    "\n",
    "Check out [this](https://docs.pola.rs/user-guide/migration/pandas/) site to see how you can convert pandas code to polars.\n",
    "\n",
    "Check out [this](https://docs.pola.rs/user-guide/ecosystem/) site for a list of libraries and tools that support polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age          workclass  fnlwgt    education  education-num  \\\n",
      "0       39          State-gov   77516    Bachelors             13   \n",
      "1       50   Self-emp-not-inc   83311    Bachelors             13   \n",
      "2       38            Private  215646      HS-grad              9   \n",
      "3       53            Private  234721         11th              7   \n",
      "4       28            Private  338409    Bachelors             13   \n",
      "...    ...                ...     ...          ...            ...   \n",
      "32556   27            Private  257302   Assoc-acdm             12   \n",
      "32557   40            Private  154374      HS-grad              9   \n",
      "32558   58            Private  151910      HS-grad              9   \n",
      "32559   22            Private  201490      HS-grad              9   \n",
      "32560   52       Self-emp-inc  287927      HS-grad              9   \n",
      "\n",
      "            marital-status          occupation    relationship    race  \\\n",
      "0            Never-married        Adm-clerical   Not-in-family   White   \n",
      "1       Married-civ-spouse     Exec-managerial         Husband   White   \n",
      "2                 Divorced   Handlers-cleaners   Not-in-family   White   \n",
      "3       Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
      "4       Married-civ-spouse      Prof-specialty            Wife   Black   \n",
      "...                    ...                 ...             ...     ...   \n",
      "32556   Married-civ-spouse        Tech-support            Wife   White   \n",
      "32557   Married-civ-spouse   Machine-op-inspct         Husband   White   \n",
      "32558              Widowed        Adm-clerical       Unmarried   White   \n",
      "32559        Never-married        Adm-clerical       Own-child   White   \n",
      "32560   Married-civ-spouse     Exec-managerial            Wife   White   \n",
      "\n",
      "           sex  capital-gain  capital-loss  hours-per-week  native-country  \\\n",
      "0         Male          2174             0              40   United-States   \n",
      "1         Male             0             0              13   United-States   \n",
      "2         Male             0             0              40   United-States   \n",
      "3         Male             0             0              40   United-States   \n",
      "4       Female             0             0              40            Cuba   \n",
      "...        ...           ...           ...             ...             ...   \n",
      "32556   Female             0             0              38   United-States   \n",
      "32557     Male             0             0              40   United-States   \n",
      "32558   Female             0             0              40   United-States   \n",
      "32559     Male             0             0              20   United-States   \n",
      "32560   Female         15024             0              40   United-States   \n",
      "\n",
      "      gross-income  \n",
      "0            <=50K  \n",
      "1            <=50K  \n",
      "2            <=50K  \n",
      "3            <=50K  \n",
      "4            <=50K  \n",
      "...            ...  \n",
      "32556        <=50K  \n",
      "32557         >50K  \n",
      "32558        <=50K  \n",
      "32559        <=50K  \n",
      "32560         >50K  \n",
      "\n",
      "[32561 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# how to read in a database into a dataframe and basic dataframe structure\n",
    "import pandas as pd\n",
    "\n",
    "# load data from a csv file\n",
    "df_pd = pd.read_csv('../data/adult_data.csv') # there are also pd.read_excel(), and pd.read_sql()\n",
    "\n",
    "print(df_pd)\n",
    "#help(df_pd.head)\n",
    "#print(df_pd.head()) # by default, shows the first five rows but check help(df+pd.head) to specify the number of rows to show\n",
    "#print(df_pd.shape) # the shape of your dataframe (number of rows, number of columns)\n",
    "#print(df_pd.shape[0]) # number of rows\n",
    "#print(df_pd.shape[1]) # number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (32_561, 15)\n",
      "┌─────┬─────────────┬────────┬─────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
      "│ age ┆ workclass   ┆ fnlwgt ┆ education   ┆ … ┆ capital-lo ┆ hours-per- ┆ native-cou ┆ gross-inco │\n",
      "│ --- ┆ ---         ┆ ---    ┆ ---         ┆   ┆ ss         ┆ week       ┆ ntry       ┆ me         │\n",
      "│ i64 ┆ str         ┆ i64    ┆ str         ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
      "│     ┆             ┆        ┆             ┆   ┆ i64        ┆ i64        ┆ str        ┆ str        │\n",
      "╞═════╪═════════════╪════════╪═════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
      "│ 39  ┆  State-gov  ┆ 77516  ┆  Bachelors  ┆ … ┆ 0          ┆ 40         ┆ United-Sta ┆  <=50K     │\n",
      "│     ┆             ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 50  ┆ Self-emp-no ┆ 83311  ┆  Bachelors  ┆ … ┆ 0          ┆ 13         ┆ United-Sta ┆  <=50K     │\n",
      "│     ┆ t-inc       ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 38  ┆  Private    ┆ 215646 ┆  HS-grad    ┆ … ┆ 0          ┆ 40         ┆ United-Sta ┆  <=50K     │\n",
      "│     ┆             ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 53  ┆  Private    ┆ 234721 ┆  11th       ┆ … ┆ 0          ┆ 40         ┆ United-Sta ┆  <=50K     │\n",
      "│     ┆             ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 28  ┆  Private    ┆ 338409 ┆  Bachelors  ┆ … ┆ 0          ┆ 40         ┆  Cuba      ┆  <=50K     │\n",
      "│ …   ┆ …           ┆ …      ┆ …           ┆ … ┆ …          ┆ …          ┆ …          ┆ …          │\n",
      "│ 27  ┆  Private    ┆ 257302 ┆  Assoc-acdm ┆ … ┆ 0          ┆ 38         ┆ United-Sta ┆  <=50K     │\n",
      "│     ┆             ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 40  ┆  Private    ┆ 154374 ┆  HS-grad    ┆ … ┆ 0          ┆ 40         ┆ United-Sta ┆  >50K      │\n",
      "│     ┆             ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 58  ┆  Private    ┆ 151910 ┆  HS-grad    ┆ … ┆ 0          ┆ 40         ┆ United-Sta ┆  <=50K     │\n",
      "│     ┆             ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 22  ┆  Private    ┆ 201490 ┆  HS-grad    ┆ … ┆ 0          ┆ 20         ┆ United-Sta ┆  <=50K     │\n",
      "│     ┆             ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "│ 52  ┆ Self-emp-in ┆ 287927 ┆  HS-grad    ┆ … ┆ 0          ┆ 40         ┆ United-Sta ┆  >50K      │\n",
      "│     ┆ c           ┆        ┆             ┆   ┆            ┆            ┆ tes        ┆            │\n",
      "└─────┴─────────────┴────────┴─────────────┴───┴────────────┴────────────┴────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# how to read in a database into a dataframe and basic dataframe structure\n",
    "import polars as pl\n",
    "\n",
    "# load data from a csv file\n",
    "df_pl = pl.read_csv('../data/adult_data.csv') # there are also pd.read_excel(), and pd.read_sql()\n",
    "# check out pl.scan_csv() too! It is useful if you know you only need to work with certain columns of the dataset\n",
    "# it will only read in the necessary columns!\n",
    "\n",
    "print(df_pl)\n",
    "#help(df_pl.head)\n",
    "#print(df_pl.head()) # by default, shows the first five rows but check help(df_pl.head) to specify the number of rows to show\n",
    "#print(df_pl.shape) # the shape of your dataframe (number of rows, number of columns)\n",
    "#print(df_pl.shape[0]) # number of rows\n",
    "#print(df_pl.shape[1]) # number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "A package is a collection of classes and functions.\n",
    "- a dataframe (pd.DataFrame()) is a pandas class\n",
    "    - a class is the blueprint of how the data should be organized \n",
    "    - classes have methods which can perform operations on the data (e.g., .head(), .shape)\n",
    "- df is an object, an instance of the class.\n",
    "    - when we put data into a class, it becomes an object \n",
    "    - methods are attached to objects \n",
    "       - you cannot call pd.head(), you can only call df.head()\n",
    "- read_csv is a function\n",
    "    - functions are called from the package\n",
    "    - you cannot call df.read_csv, you can only call pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame structure: both rows and columns are indexed!\n",
    "- index column, no name\n",
    "    - contains the row names\n",
    "    - by default, index is a range object from 0 to number of rows - 1 \n",
    "    - any column can be turned into an index, so indices can be non-number, and also non-unique. more on this later.\n",
    "- polars dataframes do not have an index column by default! rows are indexed by their integer position in the table\n",
    "    - you can add an index column if you'd like though \n",
    "- columns with column names on top in both polars and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always print your dataframe to check if it looks good!\n",
    "\n",
    "### Most common reasons it might not look ok:\n",
    "\n",
    "   - the first row is not the column name\n",
    "        - there are rows above the column names that need to be skipped\n",
    "        - there is no column name but by default, pandas assumes the first row is the column name. as a result, \n",
    "          the values of the first row end up as column names.\n",
    "   - character encoding is off\n",
    "   - separator is not comma but some other charachter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "        C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator from only the first valid\n",
      "        row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "        In addition, separators longer than 1 character and different from\n",
      "        ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "        the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "        to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, optional\n",
      "        Alias for ``sep``.\n",
      "    header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "        Row number(s) containing column labels and marking the start of the\n",
      "        data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly to ``names`` then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "        e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : Sequence of Hashable, optional\n",
      "        Sequence of column labels to apply. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : Hashable, Sequence of Hashable or False, optional\n",
      "      Column(s) to use as row label(s), denoted either by column labels or column\n",
      "      indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
      "      will be formed for the row labels.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g., when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : Sequence of Hashable or Callable, optional\n",
      "        Subset of columns to select, denoted either by column labels or column indices.\n",
      "        If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in ``names`` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "        preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "        for columns in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to ``True``. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    dtype : dtype or dict of {Hashable : dtype}, optional\n",
      "        Data type(s) to apply to either the whole dataset or individual columns.\n",
      "        E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
      "        Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "        to preserve and not interpret ``dtype``.\n",
      "        If ``converters`` are specified, they will be applied INSTEAD\n",
      "        of ``dtype`` conversion.\n",
      "    \n",
      "        .. versionadded:: 1.5.0\n",
      "    \n",
      "            Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
      "            the default determines the ``dtype`` of the columns which are not explicitly\n",
      "            listed.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "            The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict of {Hashable : Callable}, optional\n",
      "        Functions for converting values in specified columns. Keys can either\n",
      "        be column labels or column indices.\n",
      "    true_values : list, optional\n",
      "        Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
      "    false_values : list, optional\n",
      "        Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : int, list of int or Callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
      "        Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
      "        per-column ``NA`` values.  By default the following values are interpreted as\n",
      "        ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
      "        \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
      "        \"n/a\", \"nan\", \"null \".\n",
      "    \n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "        Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
      "          is appended to the default ``NaN`` values used for parsing.\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "          the default ``NaN`` values are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "          the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "          strings will be parsed as ``NaN``.\n",
      "    \n",
      "        Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
      "        ``na_values`` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "        data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "        performance of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of ``NA`` values placed in non-numeric columns.\n",
      "    \n",
      "        .. deprecated:: 2.2.0\n",
      "    skip_blank_lines : bool, default True\n",
      "        If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "    parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
      "          ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
      "        * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
      "          as a single date column. Values are joined with a space before parsing.\n",
      "        * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
      "          result 'foo'. Values are joined with a space before parsing.\n",
      "    \n",
      "        If a column or index cannot be represented as an array of ``datetime``,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an ``object`` data type. For\n",
      "        non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "        :func:`~pandas.read_csv`.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
      "        format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    \n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has no effect.\n",
      "    \n",
      "    keep_date_col : bool, default False\n",
      "        If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : Callable, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. pandas will try to call ``date_parser`` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by ``parse_dates`` into a single array\n",
      "        and pass that; and 3) call ``date_parser`` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by ``parse_dates``) as\n",
      "        arguments.\n",
      "    \n",
      "        .. deprecated:: 2.0.0\n",
      "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "           :func:`~pandas.to_datetime` as-needed.\n",
      "    date_format : str or dict of column -> format, optional\n",
      "        Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
      "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "        `strftime documentation\n",
      "        <https://docs.python.org/3/library/datetime.html\n",
      "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "        You can also pass:\n",
      "    \n",
      "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "            time string (not necessarily in exactly the same format);\n",
      "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "            and you should probably use it along with `dayfirst`.\n",
      "    \n",
      "        .. versionadded:: 2.0.0\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "    iterator : bool, default False\n",
      "        Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "        function to return a ``TextFileReader`` object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    \n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
      "        Set to ``None`` for no decompression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "    \n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "    \n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "    \n",
      "    thousands : str (length 1), optional\n",
      "        Character acting as the thousands separator in numerical values.\n",
      "    decimal : str (length 1), default '.'\n",
      "        Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character used to denote a line break. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        Character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the ``delimiter`` and it will be ignored.\n",
      "    quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "        ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
      "        characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
      "        or ``lineterminator``.\n",
      "    doublequote : bool, default True\n",
      "       When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        Character used to escape other characters.\n",
      "    comment : str (length 1), optional\n",
      "        Character indicating that the remainder of line should not be parsed.\n",
      "        If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter ``header`` but not by\n",
      "        ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "        treated as the header.\n",
      "    encoding : str, optional, default 'utf-8'\n",
      "        Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    \n",
      "    encoding_errors : str, optional, default 'strict'\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "        ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "        override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "        documentation for more details.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "    \n",
      "        - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "        - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
      "        - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "            - Callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new ``list`` of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine='python'``\n",
      "    \n",
      "        .. versionchanged:: 2.2.0\n",
      "    \n",
      "            - Callable, function with signature\n",
      "              as described in `pyarrow documentation\n",
      "              <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
      "              #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
      "    \n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to ``True``, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    \n",
      "        .. deprecated:: 2.2.0\n",
      "            Use ``sep=\"\\s+\"`` instead.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "        Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "        regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
      "        chunks. (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "        ``'legacy'`` for the original lower precision pandas converter, and\n",
      "        ``'round_trip'`` for the round-trip converter.\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "    \n",
      "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "        Back-end data type applied to the resultant :class:`DataFrame`\n",
      "        (still experimental). Behaviour is as follows:\n",
      "    \n",
      "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "          (default).\n",
      "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "          DataFrame.\n",
      "    \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_table : Read general delimited file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the help to find the solution\n",
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "The adult_test.csv file is located in the data folder as well. It is a test set of the adult dataset so you would expect the same column names and generally a similar-looking structure. Read in the file using pandas or polars in the cell below. Make sure the dataframe looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n",
      "0   25     Private  226802           11th              7        Never-married   \n",
      "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
      "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
      "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
      "4   18           ?  103497   Some-college             10        Never-married   \n",
      "\n",
      "           occupation relationship    race      sex  capital-gain  \\\n",
      "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
      "1     Farming-fishing      Husband   White     Male             0   \n",
      "2     Protective-serv      Husband   White     Male             0   \n",
      "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
      "4                   ?    Own-child   White   Female             0   \n",
      "\n",
      "   capital-loss  hours-per-week  native-country gross-income  \n",
      "0             0              40   United-States       <=50K.  \n",
      "1             0              50   United-States       <=50K.  \n",
      "2             0              40   United-States        >50K.  \n",
      "3             0              40   United-States        >50K.  \n",
      "4             0              30   United-States       <=50K.  \n"
     ]
    }
   ],
   "source": [
    "# add your pandas code below\n",
    "import pandas as pd \n",
    "df = pd.read_csv('../data/adult_test.csv', skiprows=2)\n",
    "print(df.head())\n",
    "\n",
    "# add your polars code below\n",
    "df = pl.read_csv('../data/adult_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'> Learning objectives </font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to </font>\n",
    "- <font color='LIGHTGRAY'>list main issues with data selection and collection</font>\n",
    "- <font color='LIGHTGRAY'>use pandas/polars to read in a dataset</font>\n",
    "- **filter rows of a dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter rows in pandas\n",
    "- let's assume you have one dataframe to work with but you have too much data and you need to filter out some rows\n",
    "- there are several ways to do that\n",
    "##### 1) Integer-based indexing, numpy arrays are indexed the same way.\n",
    "##### 2) Select rows based on the value of the index column\n",
    "##### 3) select rows based on column condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1) Integer-based indexing, numpy arrays are indexed the same way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# df_pd.iloc[] - for more info, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-integer\n",
    "# iloc is how numpy arrays are indexed (non-standard python indexing)\n",
    "\n",
    "# [start:stop:step] -  general indexing format\n",
    "\n",
    "# start stop step are optional\n",
    "#print(df_pd)\n",
    "#print(df_pd.iloc[:])\n",
    "#print(df_pd.iloc[::])\n",
    "#print(df_pd.iloc[::1])\n",
    "\n",
    "# select one row - 0-based indexing\n",
    "#print(df_pd.iloc[0])\n",
    "\n",
    "# indexing from the end of the data frame\n",
    "#print(df_pd.iloc[-2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# select a slice - stop index not included\n",
    "#print(df_pd.iloc[3:7])\n",
    "\n",
    "# select every second element of the slice - stop index not included\n",
    "#print(df_pd.iloc[3:7:2])\n",
    "\n",
    "#print(df_pd.iloc[3:7:-2]) # return empty dataframe\n",
    "#print(df_pd.iloc[7:3:-2])#  return rows with indices 7 and 5. 3 is the stop so it is not included\n",
    "\n",
    "# can be used to reverse rows\n",
    "#print(df_pd.iloc[::-1])\n",
    "\n",
    "# here is where indexing gets non-standard python\n",
    "# select the 2nd, 5th, and 10th rows\n",
    "#print(df_pd.iloc[[1,4,9]]) # such indexing doesn't work with lists but it works with numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### 2) Select rows based on the value of the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# df_pd.loc[] - for more info, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-label\n",
    "\n",
    "#print(df_pd.index) # the default index when reading in a file is a range index. In this case,\n",
    "                 # .loc and .iloc works ALMOST the same.\n",
    "# one difference:\n",
    "#print(df_pd.loc[3:9:2]) # this selects the 4th, 6th, 8th, 10th rows - the stop element is included!\n",
    "\n",
    "#help(df_pd.set_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age   workclass  fnlwgt    education  education-num       marital-status  \\\n",
      "age                                                                             \n",
      "30    30     Private  101135    Bachelors             13        Never-married   \n",
      "30    30     Private  229636      HS-grad              9   Married-civ-spouse   \n",
      "30    30     Private  142921   Assoc-acdm             12        Never-married   \n",
      "30    30   State-gov  260782      HS-grad              9        Never-married   \n",
      "30    30     Private  296462      HS-grad              9        Never-married   \n",
      "\n",
      "             occupation    relationship    race      sex  capital-gain  \\\n",
      "age                                                                      \n",
      "30      Exec-managerial   Not-in-family   White   Female             0   \n",
      "30    Machine-op-inspct         Husband   White     Male             0   \n",
      "30       Prof-specialty   Not-in-family   White   Female             0   \n",
      "30        Other-service   Not-in-family   White     Male             0   \n",
      "30      Exec-managerial   Not-in-family   Black     Male             0   \n",
      "\n",
      "     capital-loss  hours-per-week  native-country gross-income  \n",
      "age                                                             \n",
      "30              0              50   United-States       <=50K.  \n",
      "30              0              40          Mexico       <=50K.  \n",
      "30              0              40   United-States       <=50K.  \n",
      "30              0              40   United-States       <=50K.  \n",
      "30              0              40   United-States       <=50K.  \n"
     ]
    }
   ],
   "source": [
    "df_index_age = df_pd.set_index('age',drop=False)\n",
    "\n",
    "#print(df_pd.head())\n",
    "#print(df_index_age.head())\n",
    "#print(df_index_age.index)\n",
    "#print(df_index_age.head())\n",
    "\n",
    "print(df_index_age.loc[30].head()) # collect everyone with age 30 - the index is non-unique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3) select rows based on column condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age          workclass  fnlwgt      education  education-num  \\\n",
      "899     90            Private  149069     Assoc-acdm             12   \n",
      "2047    65            Private  444725    Prof-school             15   \n",
      "2779    55   Self-emp-not-inc  218456        Masters             14   \n",
      "3496    90   Self-emp-not-inc   83601    Prof-school             15   \n",
      "6822    44            Private  254303        Masters             14   \n",
      "6976    90            Private  250832        HS-grad              9   \n",
      "7414    90            Private  227796     Assoc-acdm             12   \n",
      "7419    90   Self-emp-not-inc  122348    Prof-school             15   \n",
      "8427    90        Federal-gov  311184        Masters             14   \n",
      "8982    90            Private  225063        HS-grad              9   \n",
      "10666   71                  ?  158437        5th-6th              3   \n",
      "10735   90          Local-gov  188242        HS-grad              9   \n",
      "11871   90                  ?   50746           10th              6   \n",
      "12437   53   Self-emp-not-inc  169112      Bachelors             13   \n",
      "12446   90            Private  347074   Some-college             10   \n",
      "13958   90            Private  272752   Some-college             10   \n",
      "15088   90            Private  197613        HS-grad              9   \n",
      "15404   27   Self-emp-not-inc  177831        HS-grad              9   \n",
      "\n",
      "               marital-status         occupation    relationship  \\\n",
      "899        Married-civ-spouse              Sales         Husband   \n",
      "2047    Married-spouse-absent       Craft-repair   Not-in-family   \n",
      "2779                 Divorced    Exec-managerial   Not-in-family   \n",
      "3496                  Widowed     Prof-specialty   Not-in-family   \n",
      "6822            Never-married     Prof-specialty   Not-in-family   \n",
      "6976       Married-civ-spouse   Transport-moving         Husband   \n",
      "7414            Never-married    Exec-managerial   Not-in-family   \n",
      "7419       Married-civ-spouse     Prof-specialty         Husband   \n",
      "8427                 Divorced     Prof-specialty   Not-in-family   \n",
      "8982       Married-civ-spouse       Craft-repair         Husband   \n",
      "10666      Married-civ-spouse                  ?         Husband   \n",
      "10735           Never-married       Craft-repair       Own-child   \n",
      "11871                Divorced                  ?   Not-in-family   \n",
      "12437      Married-civ-spouse    Exec-managerial         Husband   \n",
      "12446           Never-married       Adm-clerical       Own-child   \n",
      "13958           Never-married      Other-service       Own-child   \n",
      "15088           Never-married       Adm-clerical   Not-in-family   \n",
      "15404      Married-civ-spouse       Craft-repair         Husband   \n",
      "\n",
      "                      race      sex  capital-gain  capital-loss  \\\n",
      "899                  White     Male             0          1825   \n",
      "2047                 White     Male             0             0   \n",
      "2779                 White   Female             0             0   \n",
      "3496                 White     Male          1086             0   \n",
      "6822                 White     Male             0             0   \n",
      "6976                 White     Male          2414             0   \n",
      "7414                 White     Male          6097             0   \n",
      "7419                 White     Male         20051             0   \n",
      "8427                 White     Male             0             0   \n",
      "8982    Asian-Pac-Islander     Male             0             0   \n",
      "10666                White     Male             0             0   \n",
      "10735                White     Male         11678             0   \n",
      "11871                White   Female             0             0   \n",
      "12437                White     Male             0             0   \n",
      "12446                White   Female             0          1944   \n",
      "13958                White     Male             0             0   \n",
      "15088                White   Female             0             0   \n",
      "15404                White     Male             0             0   \n",
      "\n",
      "       hours-per-week  native-country gross-income  \n",
      "899                50   United-States        >50K.  \n",
      "2047               48         Hungary        >50K.  \n",
      "2779               50         Hungary       <=50K.  \n",
      "3496               60   United-States       <=50K.  \n",
      "6822               40         Hungary        >50K.  \n",
      "6976               40   United-States       <=50K.  \n",
      "7414               45   United-States        >50K.  \n",
      "7419               45   United-States        >50K.  \n",
      "8427               99   United-States       <=50K.  \n",
      "8982               40           South       <=50K.  \n",
      "10666              40         Hungary       <=50K.  \n",
      "10735              40   United-States        >50K.  \n",
      "11871               7   United-States       <=50K.  \n",
      "12437              40         Hungary        >50K.  \n",
      "12446              12   United-States       <=50K.  \n",
      "13958              10   United-States       <=50K.  \n",
      "15088              40   United-States        >50K.  \n",
      "15404              40         Hungary       <=50K.  \n"
     ]
    }
   ],
   "source": [
    "# one condition\n",
    "#print(df_pd[df_pd['age']==30].head())\n",
    "# here is the condition: it's a boolean series - series is basically a dataframe with one column\n",
    "#print(df_pd['age']==30)\n",
    "\n",
    "# multiple conditions can be combined with & (and) | (or)\n",
    "#print(df_pd[(df_pd['age']>30)&(df_pd['age']<35)].head())\n",
    "print(df_pd[(df_pd['age']==90)|(df_pd['native-country']==' Hungary')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter rows in polars\n",
    "### 1) Integer-based indexing\n",
    "- there are no .loc[] or .iloc[] methods in polars but you can still select rows as you would with numpy arrays\n",
    "- df_pl[start:stop:step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "df_pl[1:10:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Select rows based on column conditions\n",
    "- syntax is similar expect use .filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl.filter((df_pl['age']==30)&(df_pl['native-country']==' India'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Quiz\n",
    "How many people in adult_data.csv work at least 60 hours a week and have a doctorate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your pandas code below\n",
    "\n",
    "\n",
    "# add your polars code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data1030)",
   "language": "python",
   "name": "data1030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
